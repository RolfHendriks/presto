{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSONL To CSV Testing\n",
    "This worksheet performs a sample JSONL to CSV conversion and measures its impact on performance.\n",
    "\n",
    "This worksheet requires a large Dataset for Amazon music reviews. Before running the worksheet, please download the music review dataset from:\n",
    "\n",
    "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/CDs_and_Vinyl.jsonl.gz\n",
    "\n",
    "### Test Results / Take-Home Points:\n",
    "- CSV reduces load times by over 10x compared to JSONL - from over 5 minutes to less than half a minute for processing 5M product reviews. \n",
    "- CSV is over 3x as fast as JSON to load and 20% more space-efficient\n",
    "- File I/O is a bottleneck in Python - for unknown reasons, Python performance for file I/O is about 40x slower than raw hardware limitations\n",
    "- JSONL is not recommended for Python due to very slow speed and lack of built-in library support\n",
    "- CSV and Pandas are (barely) adequate for developer workflows on millions of records, but user-facing applications require a much more performant tech stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from jsonl_to_csv import jsonl_to_csv, profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.050:\tRead jsonl file contents\n"
     ]
    }
   ],
   "source": [
    "# I/O test: how long does it take to just read the full 3.3GB jsonl file?\n",
    "# As part of this, let's also create a separate json (not jsonl) variation that Pandas could use directly.\n",
    "\n",
    "# Result: over a minute! (76 seconds)\n",
    "# I/O speed = 3.3 GB / 76 sec = 43 MB/s\n",
    "# 5M records / 76 sec = 66K records / sec\n",
    "# Raw hardware I/O limitations dictate about 2 seconds for 3GB. It seems using Python adds *a lot* of overhead to file I/O.\n",
    "# Alternatively, perhaps the system is bound by inefficient RAM usage for files of this size.\n",
    "t = time.perf_counter()\n",
    "path = '../raw/CDs_and_Vinyl.jsonl'\n",
    "with open(path) as file:\n",
    "    jsonl = file.read()\n",
    "profile(\"Read jsonl file contents\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.165:\tparsed 100000 records\n",
      "1.561:\tparsed 200000 records\n",
      "1.998:\tparsed 300000 records\n",
      "2.473:\tparsed 400000 records\n",
      "2.969:\tparsed 500000 records\n",
      "3.212:\tparsed 600000 records\n",
      "3.804:\tparsed 700000 records\n",
      "4.071:\tparsed 800000 records\n",
      "4.793:\tparsed 900000 records\n",
      "5.041:\tparsed 1000000 records\n",
      "5.841:\tparsed 1100000 records\n",
      "6.150:\tparsed 1200000 records\n",
      "6.485:\tparsed 1300000 records\n",
      "7.844:\tparsed 1400000 records\n",
      "8.232:\tparsed 1500000 records\n",
      "8.500:\tparsed 1600000 records\n",
      "8.786:\tparsed 1700000 records\n",
      "12.253:\tparsed 1800000 records\n",
      "12.640:\tparsed 1900000 records\n",
      "12.932:\tparsed 2000000 records\n",
      "13.367:\tparsed 2100000 records\n",
      "13.710:\tparsed 2200000 records\n",
      "19.673:\tparsed 2300000 records\n",
      "20.230:\tparsed 2400000 records\n",
      "20.572:\tparsed 2500000 records\n",
      "20.882:\tparsed 2600000 records\n",
      "21.179:\tparsed 2700000 records\n",
      "21.488:\tparsed 2800000 records\n",
      "21.803:\tparsed 2900000 records\n",
      "30.885:\tparsed 3000000 records\n",
      "31.171:\tparsed 3100000 records\n",
      "31.480:\tparsed 3200000 records\n",
      "31.810:\tparsed 3300000 records\n",
      "32.120:\tparsed 3400000 records\n",
      "32.440:\tparsed 3500000 records\n",
      "32.708:\tparsed 3600000 records\n",
      "33.000:\tparsed 3700000 records\n",
      "133.273:\tparsed 3800000 records\n",
      "133.620:\tparsed 3900000 records\n",
      "133.960:\tparsed 4000000 records\n",
      "134.321:\tparsed 4100000 records\n",
      "134.648:\tparsed 4200000 records\n",
      "134.989:\tparsed 4300000 records\n",
      "135.307:\tparsed 4400000 records\n",
      "135.614:\tparsed 4500000 records\n",
      "135.932:\tparsed 4600000 records\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m line \u001b[38;5;241m:=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadline():\n\u001b[0;32m----> 9\u001b[0m         js \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m     10\u001b[0m         records\u001b[38;5;241m.\u001b[39mappend(js)\n\u001b[1;32m     11\u001b[0m         iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/decoder.py:338\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m--> 338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# JSONL test:\n",
    "# JSONL performance is erratic with bursts of speed (at the trajectory of CSV speeds in the best case) followed by long pauses.\n",
    "# Overall performance is poor, taking several minutes to parse a large file.\n",
    "# In addition, notice how the simple act of loading a DataFrame from a file becomes unexpectedly difficult.\n",
    "# This is why we need a conversion utility.\n",
    "batch_count = 100_000\n",
    "t = time.perf_counter()\n",
    "records = []\n",
    "iteration = 0\n",
    "with open(path) as file:\n",
    "    while line := file.readline():\n",
    "        js = json.loads(line)\n",
    "        records.append(js)\n",
    "        iteration += 1\n",
    "        if iteration % batch_count == 0:\n",
    "            profile(f'parsed {iteration} records', t)\n",
    "records = pd.DataFrame.from_records(records)\n",
    "profile('Finished parsing jsonl', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened file ../raw/CDs_and_Vinyl.jsonl\n",
      "6.489:\tconverted 100,000 records\n",
      "12.633:\tconverted 200,000 records\n",
      "18.929:\tconverted 300,000 records\n",
      "25.292:\tconverted 400,000 records\n",
      "31.139:\tconverted 500,000 records\n",
      "37.495:\tconverted 600,000 records\n",
      "44.252:\tconverted 700,000 records\n",
      "50.624:\tconverted 800,000 records\n",
      "57.162:\tconverted 900,000 records\n",
      "63.555:\tconverted 1,000,000 records\n",
      "70.106:\tconverted 1,100,000 records\n",
      "76.791:\tconverted 1,200,000 records\n",
      "82.984:\tconverted 1,300,000 records\n",
      "89.857:\tconverted 1,400,000 records\n",
      "96.460:\tconverted 1,500,000 records\n",
      "102.881:\tconverted 1,600,000 records\n",
      "109.713:\tconverted 1,700,000 records\n",
      "116.198:\tconverted 1,800,000 records\n",
      "122.141:\tconverted 1,900,000 records\n",
      "128.617:\tconverted 2,000,000 records\n",
      "135.181:\tconverted 2,100,000 records\n",
      "142.094:\tconverted 2,200,000 records\n",
      "148.590:\tconverted 2,300,000 records\n",
      "154.843:\tconverted 2,400,000 records\n",
      "161.129:\tconverted 2,500,000 records\n",
      "167.629:\tconverted 2,600,000 records\n",
      "174.277:\tconverted 2,700,000 records\n",
      "180.518:\tconverted 2,800,000 records\n",
      "187.005:\tconverted 2,900,000 records\n",
      "193.135:\tconverted 3,000,000 records\n",
      "199.229:\tconverted 3,100,000 records\n",
      "205.869:\tconverted 3,200,000 records\n",
      "212.573:\tconverted 3,300,000 records\n",
      "218.990:\tconverted 3,400,000 records\n",
      "225.542:\tconverted 3,500,000 records\n",
      "231.676:\tconverted 3,600,000 records\n",
      "238.246:\tconverted 3,700,000 records\n",
      "244.828:\tconverted 3,800,000 records\n",
      "251.110:\tconverted 3,900,000 records\n",
      "257.825:\tconverted 4,000,000 records\n",
      "264.598:\tconverted 4,100,000 records\n",
      "271.076:\tconverted 4,200,000 records\n",
      "277.173:\tconverted 4,300,000 records\n",
      "283.726:\tconverted 4,400,000 records\n",
      "290.478:\tconverted 4,500,000 records\n",
      "296.944:\tconverted 4,600,000 records\n",
      "303.750:\tconverted 4,700,000 records\n",
      "310.156:\tconverted 4,800,000 records\n",
      "312.087:\tFinshed converting 4827273 records from json to csv.\n",
      "See None for results.\n"
     ]
    }
   ],
   "source": [
    "# Conversion test: 5 minutes 21 seconds.\n",
    "# We can use this as a proxy for roughly how long it would take to read a dataset from a jsonl file if we were successful.\n",
    "jsonl_to_csv(path, batch_size = 100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.514:\tRead CSV file\n"
     ]
    }
   ],
   "source": [
    "# CSV variation:\n",
    "# - Performance = 25 seconds - over 3x as fast as json and over 10x as fast as jsonl\n",
    "#   - Though this bridges the performance gap for developers, it also shows that CSV files are not at all adequate for user-facing workflows on real-world product data\n",
    "# - Storage space decreases by 20% - down from 3.3GB to 2.6GB\n",
    "csv_path = path.replace('.jsonl', '.csv')\n",
    "t = time.perf_counter()\n",
    "data = pd.read_csv(csv_path)\n",
    "profile('Read CSV file', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = path.replace('.jsonl', '.json')\n",
    "json_str = '[' + jsonl + ']'\n",
    "with open(json_path, 'w') as file:\n",
    "    file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON variation:\n",
    "# 75 seconds to load records. Almost exactly the same speed as dumping the file into a string, and 3x slower than CSV.\n",
    "t = time.perf_counter()\n",
    "data = pd.read_json(json_path)\n",
    "profile('Read json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

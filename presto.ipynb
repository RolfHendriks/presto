{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a92cc85-1fde-434d-97d7-c22998a8803a",
   "metadata": {},
   "source": [
    "# MusicMate\n",
    "\n",
    "### Amazon Music Reviews\n",
    "\n",
    "### Pitchfork Reviews\n",
    "- https://www.kaggle.com/datasets/nolanbconaway/pitchfork-data\n",
    "\n",
    "### Spotify\n",
    "\n",
    "Kaggle datasets have:\n",
    "\n",
    "ðŸŽ¤ Artist: The name of the artist who performed the song.\n",
    "\n",
    "ðŸŽµ Song: The title of the song.\n",
    "\n",
    "â±ï¸ Duration (ms): The length of the song in milliseconds.\n",
    "\n",
    "ðŸ”ž Explicit: Indicates whether the song contains explicit content (True/False).\n",
    "\n",
    "ðŸ“… Year: The release year of the song.\n",
    "\n",
    "ðŸ“ˆ Popularity: A score reflecting the song's popularity.\n",
    "\n",
    "ðŸ•º Danceability: A measure of how suitable the song is for dancing.\n",
    "\n",
    "âš¡ Energy: A measure of the song's intensity and activity.\n",
    "\n",
    "ðŸŽ¼ Key: The musical key in which the song is composed.\n",
    "\n",
    "ðŸ”Š Loudness: The overall loudness of the song in decibels.\n",
    "\n",
    "ðŸŽšï¸ Mode: Indicates the modality (major or minor) of the song.\n",
    "\n",
    "ðŸ—£ï¸ Speechiness: The presence of spoken words in the track.\n",
    "\n",
    "ðŸŽ¸ Acousticness: A measure of the acoustic sound of the song.\n",
    "\n",
    "ðŸŽ¹ Instrumentalness: Predicts whether the track contains no vocals.\n",
    "\n",
    "ðŸŽ¤ Liveness: The probability that the track was performed live.\n",
    "\n",
    "ðŸ˜Š Valence: The musical positiveness conveyed by the song.\n",
    "\n",
    "ðŸŽ§ Tempo: The tempo of the song in beats per minute (BPM).\n",
    "\n",
    "ðŸŽ¶ Genre: The genre(s) of the song.\n",
    "\n",
    "\n",
    "#### Spotify Developer API\n",
    "- https://developer.spotify.com/documentation/web-api\n",
    "- Artist:\n",
    "    - In: artist IDs (not names). Can look up one or multiple\n",
    "        - There does not seem to be a way to lookup ID by name. Seems like a deal-breaker.\n",
    "    - Out: genres, images, popularity\n",
    "    - Separate APIs: artist albums, artist tracks\n",
    "- Album:\n",
    "    - Artists\n",
    "    - Tracks\n",
    "    - Genres\n",
    "    - Images\n",
    "    - Popularity  \n",
    "\n",
    "#### Apple Music API\n",
    "- https://developer.apple.com/documentation/applemusicapi\n",
    "- Inputs: artist ID, region\n",
    "    - How to get these based on name?\n",
    "- Can specify one of several views. Most notably:\n",
    "    - full albums\n",
    "    - singles\n",
    "- Determining what data is returned is not at all transparent from documentation\n",
    "\n",
    "#### Amazon API\n",
    "- No apparent documentation for mining data from Amazon products + reviews\n",
    "    - Dev site saturated with AWS and Alexa development\n",
    "\n",
    "#### Amazon Reviews Datasets\n",
    "- Full Score Dataset:\n",
    "    - https://figshare.com/articles/dataset/Amazon_Reviews_Full/13232537/1?file=25483985 for description + details\n",
    "    - https://s3.amazonaws.com/fast-ai-nlp/amazon_review_full_csv.tgz for raw download\n",
    "    - Has Ratings + reviews, but no product info. So useless for the sake of this project\n",
    "    - Seminal paper: **Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text - Julian McAuley & Jure Leskovec - Stanford University - 2013**\n",
    "        - 33M user reviews (how?!)\n",
    "        - But not accessible\n",
    "- Kaggle:\n",
    "    - McAuley latest review data:\n",
    "        - https://www.kaggle.com/datasets/wajahat1064/amazon-reviews-data-2023\n",
    "            - 5M |700K | 1.8M song reviews | products | users\n",
    "            - 29.5M books\n",
    "            - 17.3M movies\n",
    "            - 71K | 3.4K | 60K magazine reviews | products | users \n",
    "        - Per-category review and product data!\n",
    "        - Mixes Audiobook data with song data\n",
    "        - Missing genre data\n",
    "        - Artist is not spelled out, but can be extracted from 'store' metadata\n",
    "    - **Book** reviews dataset:\n",
    "        - https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
    "        - 3M reviews of 212K books with complete metadata. An excellent data set!\n",
    "\n",
    "\n",
    "#### MusicBrainz\n",
    "- No-fluff SQL database with thin web/API layer. Searching Scandroid leads to impressive and up-to-date results!\n",
    "- Data dump: https://musicbrainz.org/doc/Development/JSON_Data_Dumps\n",
    "- **CritiqueBrainz** data dump of user reviews!\n",
    "    - 13K reviews by 5K users, according to home page. Still has a long way to go to be usable. \n",
    "\n",
    "#### Million Song Dataset\n",
    "- millionsongdataset.com\n",
    "- 300GB (!)\n",
    "- Contains links to comprehensive lists of genres/artists\n",
    "- Does not appear to be up-to-date. Cannot find most current (and not-so-current) artists: Tom Macdonald, VNV Nation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f29175b-1342-4eed-a246-e84839e363a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import dateutil\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "t = time.perf_counter()\n",
    "def profile(message, timestamp = t):\n",
    "    elapsed = time.perf_counter() - timestamp\n",
    "    print(f'{elapsed:.3f}:\\t{message}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023ab59-37bd-4233-9270-e164146b6b49",
   "metadata": {},
   "source": [
    "### ETL - Amazon Digital Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d109b69f-82ed-440a-bfa1-30b68b9a9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.perf_counter()\n",
    "\n",
    "def parse_amazon_review(review: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given an Amazon user review from Kaggle data, return a review in a normalized format.\n",
    "    \"\"\"\n",
    "    # Review data schema:\n",
    "    # - rating\n",
    "    # - title\n",
    "    # - text: user review text\n",
    "    # - parent_asin: product id\n",
    "    # - asin: redundant product id\n",
    "    # - user_id\n",
    "    # - helpful_vote: number of upvotes for this review\n",
    "    # - timestamp\n",
    "    # - images\n",
    "    # - verified_purchase\n",
    "    columns = ['rating', 'title', 'text', 'parent_asin', 'user_id', 'helpful_vote', 'timestamp']\n",
    "    result = review[columns]\n",
    "    # manufacture a unique ID based on product + user\n",
    "    result['id'] = result.parent_asin + '-' + result.user_id\n",
    "    # use normalized format for review data\n",
    "    return result.rename({\n",
    "        'text': 'review',\n",
    "        'parent_asin': 'product_id'\n",
    "    })\n",
    "    # intentionally removed:\n",
    "    # - images\n",
    "    # - verified_purchase\n",
    "    # - asin (using parent_asin instead)\n",
    "\n",
    "def parse_amazon_product(product: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given an Amazon product description from Kaggle data, return a product description in a normalized format.\n",
    "    \"\"\"\n",
    "    # Product data schema:\n",
    "    # - main_category: music, books, etc\n",
    "    # - title: album/product title\n",
    "    # - average_rating: combined average user review rating\n",
    "    # - rating_number: user review count\n",
    "    # - features: miscellaneous notes for some rare/collectors CDs for example\n",
    "    # - description: product description text\n",
    "    # - price\n",
    "    # - images\n",
    "    # - store: contains combination of artist and audio format\n",
    "    # - categories: ?\n",
    "    # - details: dictionary with potentially large volumes of arbitrary metadata. Usually contains album release date. Catalog includes non-music items that can be identified by eccentric metadata if needed.\n",
    "    # - parent_asin: product id\n",
    "    # - bought_together: not used\n",
    "    columns = ['parent_asin', 'title', 'description', 'images', 'main_category']\n",
    "    result = product[columns]\n",
    "    if date := product['details'].get('Date First Available'):\n",
    "        result['date'] = date\n",
    "    return result.rename({\n",
    "        'parent_asin': 'id',\n",
    "        'main_category': 'type',\n",
    "    })\n",
    "\n",
    "def load_amazon_reviews(path, verbosity = 0) -> pd.DataFrame:\n",
    "    with open(path) as file:\n",
    "        records = []\n",
    "        while (line := file.readline()):\n",
    "            record = pd.Series(json.loads(line))\n",
    "            record = parse_amazon_review(record)\n",
    "            records.append(record)\n",
    "            if verbosity > 0 and len(records) % 10_000 == 0:\n",
    "                print(len(records))\n",
    "        result = pd.DataFrame.from_records(records)\n",
    "        result.set_index('id', inplace = True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f81f9e-654b-4813-8ec9-e08bbf40f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#music_reviews = load_amazon_reviews('Digital_Music.jsonl', verbosity = 1)\n",
    "#profile(f'Read {len(music_reviews)} digital music reviews')\n",
    "#music_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "577aee11-2705-4e4e-aea1-17996f97d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.035:\tconverted 10,000 records\n",
      "5.888:\tconverted 20,000 records\n",
      "8.542:\tconverted 30,000 records\n",
      "11.395:\tconverted 40,000 records\n",
      "14.242:\tconverted 50,000 records\n",
      "17.082:\tconverted 60,000 records\n",
      "19.737:\tconverted 70,000 records\n",
      "22.559:\tconverted 80,000 records\n",
      "25.391:\tconverted 90,000 records\n",
      "28.046:\tconverted 100,000 records\n",
      "30.875:\tconverted 110,000 records\n",
      "33.696:\tconverted 120,000 records\n",
      "36.541:\tconverted 130,000 records\n",
      "36.656:\tFinshed converting 130434 records\n",
      "107.398:\tConverted digital music json to csv\n"
     ]
    }
   ],
   "source": [
    "def append_recordz(records: pd.DataFrame, path: str, index_col = 'id', verbosity = 1) -> int:\n",
    "    try:\n",
    "        old_records = pd.read_csv(path, index_col = index_col)\n",
    "        if verbosity > 0:\n",
    "            print(f'{len(old_records)} old records')\n",
    "    except:\n",
    "        old_records = None\n",
    "    new_records = records\n",
    "    if old_records is not None:\n",
    "        new_record_ids = new_records.index.difference(old_records.index)\n",
    "        new_records = new_records.loc[new_record_ids]\n",
    "    if len(new_records) > 0:\n",
    "        new_records = pd.concat([old_records, new_records])\n",
    "        new_records.to_csv(path)\n",
    "    if verbosity > 0:\n",
    "        print(f'{len(new_records)} new records')\n",
    "    return len(new_records)\n",
    "\n",
    "def append_records(records: pd.DataFrame, path: str, index_col = 'id', verbosity = 1) -> int:\n",
    "    with open(path, 'a') as outfile:\n",
    "        records.to_csv(outfile)\n",
    "        if verbosity > 0:\n",
    "            print(f'appended {len(records)} records to {path}')\n",
    "    return len(records)\n",
    "    # possible future iteration as needed: prevent duplicates\n",
    "\n",
    "def jsonl_to_csv(jsonpath: str, process_json = lambda x: x, index_col = 'id', batch_size = 10_000, verbosity = 1):\n",
    "    csv_filename = jsonpath.replace('jsonl', 'csv')\n",
    "    if os.path.exists(csv_filename):\n",
    "        if verbosity > 0:\n",
    "            print('CSV file already exists. Skipping.')\n",
    "        return        \n",
    "    t0 = time.perf_counter()\n",
    "    with open(jsonpath) as file:\n",
    "\n",
    "        batch = []  \n",
    "        count = 0\n",
    "        def append_batch() -> int:\n",
    "            records = pd.DataFrame.from_records(batch)\n",
    "            return append_records(records, csv_filename, index_col = index_col, verbosity = 0)\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if line:\n",
    "                js = json.loads(line)\n",
    "                record = process_json(pd.Series(js))\n",
    "                batch.append(record)  \n",
    "                if len(batch) == batch_size:\n",
    "                    count += append_batch()\n",
    "                    batch = []\n",
    "                    if verbosity > 0:\n",
    "                        profile(f'converted {count:,} records', t0)\n",
    "            else:\n",
    "                count += append_batch()\n",
    "                if verbosity > 0:\n",
    "                    profile(f'Finshed converting {count} records', t0)\n",
    "                break\n",
    "\n",
    "t = time.perf_counter()\n",
    "jsonl_to_csv('Digital_Music.jsonl', process_json = parse_amazon_review)\n",
    "profile(f'Converted digital music json to csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "891202e6-92b0-4ad6-8bb6-0b86352c1e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.020:\tconverted 10,000 records\n",
      "5.618:\tconverted 20,000 records\n",
      "8.407:\tconverted 30,000 records\n",
      "11.195:\tconverted 40,000 records\n",
      "14.018:\tconverted 50,000 records\n",
      "16.629:\tconverted 60,000 records\n",
      "19.458:\tconverted 70,000 records\n",
      "19.622:\tFinshed converting 70537 records\n",
      "19.623:\tConverted digital music product json to csv\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "jsonl_to_csv('meta_Digital_Music.jsonl', process_json = parse_amazon_product)\n",
    "profile(f'Converted digital music product json to csv', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79c732cf-088d-48cc-be6d-2ac959d867d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.007:\tconverted 10,000 records\n",
      "5.679:\tconverted 20,000 records\n",
      "8.528:\tconverted 30,000 records\n",
      "11.405:\tconverted 40,000 records\n",
      "14.285:\tconverted 50,000 records\n",
      "17.204:\tconverted 60,000 records\n",
      "19.888:\tconverted 70,000 records\n",
      "22.763:\tconverted 80,000 records\n",
      "25.633:\tconverted 90,000 records\n",
      "28.537:\tconverted 100,000 records\n",
      "31.217:\tconverted 110,000 records\n",
      "34.146:\tconverted 120,000 records\n",
      "37.042:\tconverted 130,000 records\n",
      "39.944:\tconverted 140,000 records\n",
      "42.643:\tconverted 150,000 records\n",
      "47.034:\tconverted 160,000 records\n",
      "50.005:\tconverted 170,000 records\n",
      "53.009:\tconverted 180,000 records\n",
      "55.699:\tconverted 190,000 records\n",
      "58.569:\tconverted 200,000 records\n",
      "61.445:\tconverted 210,000 records\n",
      "64.125:\tconverted 220,000 records\n",
      "67.023:\tconverted 230,000 records\n",
      "69.921:\tconverted 240,000 records\n",
      "72.822:\tconverted 250,000 records\n",
      "75.522:\tconverted 260,000 records\n",
      "78.452:\tconverted 270,000 records\n",
      "81.361:\tconverted 280,000 records\n",
      "84.248:\tconverted 290,000 records\n",
      "87.121:\tconverted 300,000 records\n",
      "89.810:\tconverted 310,000 records\n",
      "92.721:\tconverted 320,000 records\n",
      "95.574:\tconverted 330,000 records\n",
      "98.359:\tconverted 340,000 records\n",
      "100.939:\tconverted 350,000 records\n",
      "103.697:\tconverted 360,000 records\n",
      "106.545:\tconverted 370,000 records\n",
      "109.471:\tconverted 380,000 records\n",
      "112.110:\tconverted 390,000 records\n",
      "114.977:\tconverted 400,000 records\n",
      "117.732:\tconverted 410,000 records\n",
      "120.569:\tconverted 420,000 records\n",
      "123.223:\tconverted 430,000 records\n",
      "126.135:\tconverted 440,000 records\n",
      "129.032:\tconverted 450,000 records\n",
      "131.930:\tconverted 460,000 records\n",
      "134.848:\tconverted 470,000 records\n",
      "137.557:\tconverted 480,000 records\n",
      "140.480:\tconverted 490,000 records\n",
      "143.333:\tconverted 500,000 records\n",
      "145.976:\tconverted 510,000 records\n",
      "148.795:\tconverted 520,000 records\n",
      "151.644:\tconverted 530,000 records\n",
      "154.499:\tconverted 540,000 records\n",
      "157.348:\tconverted 550,000 records\n",
      "160.059:\tconverted 560,000 records\n",
      "162.958:\tconverted 570,000 records\n",
      "165.867:\tconverted 580,000 records\n",
      "168.813:\tconverted 590,000 records\n",
      "171.467:\tconverted 600,000 records\n",
      "174.362:\tconverted 610,000 records\n",
      "177.200:\tconverted 620,000 records\n",
      "180.037:\tconverted 630,000 records\n",
      "182.679:\tconverted 640,000 records\n",
      "185.527:\tconverted 650,000 records\n",
      "188.370:\tconverted 660,000 records\n",
      "191.222:\tconverted 670,000 records\n",
      "193.865:\tconverted 680,000 records\n",
      "196.693:\tconverted 690,000 records\n",
      "199.581:\tconverted 700,000 records\n",
      "200.112:\tFinshed converting 701959 records\n",
      "200.120:\tConverted CD product json to csv\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "jsonl_to_csv('meta_CDs_and_Vinyl.jsonl', process_json = parse_amazon_product)\n",
    "profile(f'Converted CD product json to csv', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8e46d11-2bf0-4e83-93fa-6e695f78aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.111:\tconverted 100,000 records\n",
      "56.003:\tconverted 200,000 records\n",
      "83.752:\tconverted 300,000 records\n",
      "112.357:\tconverted 400,000 records\n",
      "140.186:\tconverted 500,000 records\n",
      "168.135:\tconverted 600,000 records\n",
      "196.043:\tconverted 700,000 records\n",
      "224.114:\tconverted 800,000 records\n",
      "252.430:\tconverted 900,000 records\n",
      "280.327:\tconverted 1,000,000 records\n",
      "308.171:\tconverted 1,100,000 records\n",
      "336.806:\tconverted 1,200,000 records\n",
      "366.609:\tconverted 1,300,000 records\n",
      "394.490:\tconverted 1,400,000 records\n",
      "422.512:\tconverted 1,500,000 records\n",
      "450.313:\tconverted 1,600,000 records\n",
      "478.285:\tconverted 1,700,000 records\n",
      "506.417:\tconverted 1,800,000 records\n",
      "534.203:\tconverted 1,900,000 records\n",
      "561.964:\tconverted 2,000,000 records\n",
      "590.238:\tconverted 2,100,000 records\n",
      "618.230:\tconverted 2,200,000 records\n",
      "646.511:\tconverted 2,300,000 records\n",
      "674.545:\tconverted 2,400,000 records\n",
      "702.208:\tconverted 2,500,000 records\n",
      "730.380:\tconverted 2,600,000 records\n",
      "758.509:\tconverted 2,700,000 records\n",
      "786.678:\tconverted 2,800,000 records\n",
      "814.577:\tconverted 2,900,000 records\n",
      "842.419:\tconverted 3,000,000 records\n",
      "870.223:\tconverted 3,100,000 records\n",
      "898.251:\tconverted 3,200,000 records\n",
      "926.185:\tconverted 3,300,000 records\n",
      "954.476:\tconverted 3,400,000 records\n",
      "982.360:\tconverted 3,500,000 records\n",
      "1010.505:\tconverted 3,600,000 records\n",
      "1038.468:\tconverted 3,700,000 records\n",
      "1066.109:\tconverted 3,800,000 records\n",
      "1094.298:\tconverted 3,900,000 records\n",
      "1122.522:\tconverted 4,000,000 records\n",
      "1150.811:\tconverted 4,100,000 records\n",
      "1178.936:\tconverted 4,200,000 records\n",
      "1206.801:\tconverted 4,300,000 records\n",
      "1234.678:\tconverted 4,400,000 records\n",
      "1262.462:\tconverted 4,500,000 records\n",
      "1290.461:\tconverted 4,600,000 records\n",
      "1318.759:\tconverted 4,700,000 records\n",
      "1346.471:\tconverted 4,800,000 records\n",
      "1354.112:\tFinshed converting 4827273 records\n",
      "1354.144:\tConverted CD review json to csv\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "jsonl_to_csv('CDs_and_Vinyl.jsonl', process_json = parse_amazon_review, batch_size = 100_000)\n",
    "profile(f'Converted CD review json to csv', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0fe373ca-9227-4623-93c4-1587efda8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(path = 'reviews.csv') -> pd.DataFrame:\n",
    "    #result = pd.read_csv(path, index_col = 'id')\n",
    "    #result.rating = result.rating.astype(np.float16)\n",
    "    #result.helpful_vote = result.helpful_vote.astype(np.int)\n",
    "    #result.timestamp = result.timestamp.astype(np.long)\n",
    "    #return result\n",
    "    return pd.read_csv(path, index_col = 'id')\n",
    "    # why does this not work?\n",
    "    return pd.read_csv(path, index_col = 'id', dtype = {\n",
    "        'rating': np.float16,\n",
    "        'timestamp': np.int64,\n",
    "        'helpful_vote': np.int32,\n",
    "    })\n",
    "\n",
    "def save_reviews(reviews: pd.DataFrame, path = 'reviews.csv'):\n",
    "    append_records(reviews, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c753362e-e574-439c-810e-81bb66a93476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/dkqc84017pz9hvxv5jt5xfg00000gn/T/ipykernel_13804/3097498378.py:7: DtypeWarning: Columns (1,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(path, index_col = 'id')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.737:\tRead CD reviews from CSV\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "reviews = load_reviews('CDs_and_Vinyl.csv')\n",
    "profile('Read CD reviews from CSV', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b36c4b57-7de6-4a0d-bc4b-31585668162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.839:\tCombined CD and Digital Music Reviews\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4957768, 8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "reviews = pd.concat([\n",
    "    reviews,\n",
    "    load_reviews('Digital_Music.csv')\n",
    "])\n",
    "profile('Combined CD and Digital Music Reviews', t)\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7203f5c7-0f6a-4bf0-a26d-b479f28d60f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "5.0       2396867\n",
       "5.0       1254535\n",
       "4.0        443622\n",
       "4.0        235021\n",
       "3.0        190363\n",
       "1.0        128405\n",
       "3.0         99720\n",
       "2.0         94514\n",
       "1.0         65458\n",
       "2.0         49202\n",
       "rating         61\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "830b061e-8fb1-4442-8a11-6464b86f31f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>product_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rating</td>\n",
       "      <td>title</td>\n",
       "      <td>review</td>\n",
       "      <td>product_id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>timestamp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  rating  title  review  product_id  user_id  helpful_vote  \\\n",
       "id                                                                         \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "..         ...     ...    ...     ...         ...      ...           ...   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "id         NaN  rating  title  review  product_id  user_id  helpful_vote   \n",
       "\n",
       "    timestamp  \n",
       "id             \n",
       "id  timestamp  \n",
       "id  timestamp  \n",
       "id  timestamp  \n",
       "id  timestamp  \n",
       "id  timestamp  \n",
       "..        ...  \n",
       "id  timestamp  \n",
       "id  timestamp  \n",
       "id  timestamp  \n",
       "id  timestamp  \n",
       "id  timestamp  \n",
       "\n",
       "[61 rows x 8 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[reviews.rating == 'rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "269c846c-0600-4e2f-bfc6-ee8aef988fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5088215 entries, B002MW50JA-AGKASBHYZPGTEPO6LWZPVJWB2BVA to B00000853T-AEFCHGMHFSZA4IWC5FWTBRPR25GQ\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   Unnamed: 0    float64\n",
      " 1   rating        object \n",
      " 2   title         object \n",
      " 3   review        object \n",
      " 4   product_id    object \n",
      " 5   user_id       object \n",
      " 6   helpful_vote  object \n",
      " 7   timestamp     object \n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 349.4+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d94ef-5741-4b96-a9b2-8d6aaed519f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_products(path = 'products.csv') -> pd.DataFrame:\n",
    "    result = pd.read_csv(path, index_col = 'id')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "339f78b4-6af0-49f4-9533-7be02b22640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.556:\tRead CD product data\n",
      "4.892:\tCombined CD and Digital Music product data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(772573, 7)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "products = pd.read_csv('meta_CDs_and_Vinyl.csv')\n",
    "profile('Read CD product data', t)\n",
    "products = pd.concat([\n",
    "    products,\n",
    "    pd.read_csv('meta_Digital_Music.csv')\n",
    "])\n",
    "profile('Combined CD and Digital Music product data', t)\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa7eb98-4157-4e2e-b54e-84805ea2d22a",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d1ab9b6-b80e-4494-aa2d-39973dee0d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist('Scandroid Format: Audio CD') = 'Scandroid'\n",
      "artist('Scandroid (Artist) Format: Audio CD') = 'Scandroid'\n",
      "artist('Beethoven (Composer), Berliner Philharmonika (Performer)') = 'Beethoven'\n"
     ]
    }
   ],
   "source": [
    "def extract_artist(text: str) -> str:\n",
    "    result = text.partition('Format:')[0]\n",
    "    try:\n",
    "        i = result.index('(')\n",
    "        result = result[:i]\n",
    "    except:\n",
    "        pass\n",
    "    return result.strip()\n",
    "\n",
    "examples = [\n",
    "    \"Scandroid Format: Audio CD\",\n",
    "    \"Scandroid (Artist) Format: Audio CD\",\n",
    "    \"Beethoven (Composer), Berliner Philharmonika (Performer)\",\n",
    "]\n",
    "for example in examples:\n",
    "    artist = extract_artist(example)\n",
    "    print(f\"artist('{example}') = '{artist}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6653ce2e-90f4-410e-8b82-0c8915056af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize('The Who') = 'who'\n",
      "normalize('Who') = 'who'\n",
      "normalize('Who, The') = 'who'\n",
      "normalize('R.E.M') = 'rem'\n",
      "normalize('R E M') = 'rem'\n",
      "normalize('RMB') = 'rmb'\n",
      "normalize('CÃ©line Dion') = 'celinedion'\n",
      "normalize('BlÃ¼mchen') = 'blumchen'\n",
      "normalize('Bluemchen') = 'bluemchen'\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    result = text.casefold().replace('the', '')\n",
    "    result = re.sub(r'[^\\w]', '', result)\n",
    "    result = unidecode.unidecode(result)\n",
    "    return result\n",
    "\n",
    "examples = [\n",
    "    'The Who', 'Who', 'Who, The',\n",
    "    'R.E.M', 'R E M',\n",
    "    'RMB',\n",
    "    'CÃ©line Dion', 'BlÃ¼mchen', 'Bluemchen',\n",
    "]\n",
    "for example in examples:\n",
    "    norm = normalize(example)\n",
    "    print(f\"normalize('{example}') = '{norm}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0188da-d793-4f62-bb4b-ddbda1fcfb74",
   "metadata": {},
   "source": [
    "## Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735cdbb-c7cb-400a-8666-2f940e83d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_products(meta: pd.DataFrame, count = 10) -> pd.DataFrame:\n",
    "    return meta.sort_values(by = 'rating_number', ascending = False)[:count]\n",
    "\n",
    "def process_products(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    result = data.copy()\n",
    "    result = result.loc[result.store.dropna().index]\n",
    "    if 'store' in result.columns:\n",
    "        result['artist'] = result.store.map(extract_artist)\n",
    "        result['artist_norm'] = result.artist.map(normalize)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ac65d-bc8f-4445-83cc-c9f3230bdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "products = read_json(meta_file, meta_limit)\n",
    "products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968606a-5f6f-4ee7-a395-44899337652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = process_products(products)\n",
    "best_products(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba7c14-284f-4df5-b57e-74fa110fc61b",
   "metadata": {},
   "source": [
    "### Artist Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b3019-32fa-4f55-a839-fa788fd5754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_artists.txt') as file:\n",
    "    all_artists = [artist for artist in file]\n",
    "all_artists = pd.DataFrame(all_artists)\n",
    "all_artists.columns = ['artist']\n",
    "all_artists['artist_norm'] = all_artists.artist.map(normalize)\n",
    "all_artists_set = set(all_artists.artist_norm)\n",
    "\n",
    "def filter_artists(artists: pd.Series, return_removed = False):\n",
    "    norm = artists.map(normalize)\n",
    "    is_artist = norm.map(lambda artist: artist in all_artists_set)\n",
    "    results = artists[is_artist]\n",
    "    if return_removed:\n",
    "        non_artists = artists[is_artist == False]\n",
    "        return (results, non_artists)\n",
    "    return results\n",
    "\n",
    "\n",
    "products.artist.value_counts()[:20]\n",
    "#products.groupby('artist')['title'].count().sort_values(ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5522051-8a2f-4283-9a44-702fa4eb72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists, non_artists = filter_artists(products.artist, return_removed = True)\n",
    "len(artists), len(non_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9a331-a27c-4a9e-9131-19f658c652e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_artists.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbfb65-52f7-406a-a94e-93dbf7450033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Billboard Hot 100 at time of writing\n",
    "new_artists = [\n",
    "    'Shaboozey', 'Lady Gaga', 'Billie Eilish', 'Sabrina Carpenter', 'Teddy Swims', 'Tyler, The Creator'\n",
    "]\n",
    "# Eclectic niche artists:\n",
    "niche_artists = [\n",
    "    'Scandroid', 'Zombie Hyperdrive', 'Dynatron', 'Magic Sword', 'Dance With the Dead', # Synthwave\n",
    "    'Eisfabrik', 'Apoptygma Berzerk', # Industrial Dance\n",
    "    'Zircon', 'Rushjet1', 'Megadrive', # Chiptunes\n",
    "    'Alice in Videoland', 'Thermostatic', 'Parralox', 'Pool Waitress', # Synthpop\n",
    "    'Juno Reactor', 'BlÃ¼mchen', # misc\n",
    "]\n",
    "\n",
    "test_artists = new_artists + niche_artists\n",
    "results = []\n",
    "for artist in test_artists:\n",
    "    norm = normalize(artist)\n",
    "    in_db = norm in all_artists_set\n",
    "    in_products = norm in product_artists_set\n",
    "    results.append({'in_db': in_db, 'in_products':in_products})\n",
    "\n",
    "pd.DataFrame.from_records(results, index = test_artists)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae2209-2d25-47f2-bc01-0ab39e0e680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Products + Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2de1dd21-edbf-4e37-a1ac-6aace44a1ec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns for key 'product_id'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#def product_reviews(reviews: pd.DataFrame, products: pd.DataFrame) -> pd.DataFrame:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#    reviews.join(products, on = 'product_id')\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m product_reviews \u001b[38;5;241m=\u001b[39m reviews\u001b[38;5;241m.\u001b[39mjoin(products, on \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m product_reviews\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:10757\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[1;32m  10747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m  10748\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m  10749\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10750\u001b[0m             other,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10755\u001b[0m             validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m  10756\u001b[0m         )\n\u001b[0;32m> 10757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m  10758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10759\u001b[0m         other,\n\u001b[1;32m  10760\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m  10761\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[1;32m  10762\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mon \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10763\u001b[0m         right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m  10764\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39m(lsuffix, rsuffix),\n\u001b[1;32m  10765\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m  10766\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m  10767\u001b[0m     )\n\u001b[1;32m  10768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m  10769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    171\u001b[0m         left_df,\n\u001b[1;32m    172\u001b[0m         right_df,\n\u001b[1;32m    173\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[1;32m    174\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m    175\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[1;32m    176\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[1;32m    177\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[1;32m    178\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[1;32m    179\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    180\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[1;32m    181\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[1;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1508\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1504\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1505\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1506\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1507\u001b[0m     ):\n\u001b[0;32m-> 1508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on object and int64 columns for key 'product_id'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "def joinz(reviews: pd.DataFrame, metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    # This should be trivial:\n",
    "    #return reviews.join(meta, on = 'id', how = 'left') \n",
    "    # But for unknown reasons Pandas cannot handle this data. Let's perform a manual join.\n",
    "    result = reviews.copy()\n",
    "    new_columns = meta.columns.difference(reviews.columns)\n",
    "    print(new_columns)\n",
    "    for column in new_columns:\n",
    "        result[column] = None\n",
    "    for review_id in reviews.parent_asin:\n",
    "        review = result.loc[review_id]\n",
    "        try:\n",
    "            data = meta.loc[review_id]\n",
    "            for column in new_columns:\n",
    "                review[column] = data.column\n",
    "                print(f'review[{column}] = {data.column}')\n",
    "        except:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "#def product_reviews(reviews: pd.DataFrame, products: pd.DataFrame) -> pd.DataFrame:\n",
    "#    reviews.join(products, on = 'product_id')\n",
    "\n",
    "product_reviews = reviews.join(products, on = 'product_id', how = 'left')\n",
    "product_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ecc424a-e4a2-4e02-b75c-c7d2646cdf17",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tosql'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b1/dkqc84017pz9hvxv5jt5xfg00000gn/T/ipykernel_13804/2765456836.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtosql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'products.sql'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tosql'"
     ]
    }
   ],
   "source": [
    "products.to_sql('products.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03edfba-e921-42a4-904e-f2089353673d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
